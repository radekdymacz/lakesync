---
title: Architecture
description: System architecture, key patterns, and design decisions.
---

## Overview

LakeSync syncs any data source to a local working set. The core loop follows a **push/pull** model where consumers (web apps, AI agents) push local changes and pull filtered subsets from remote sources via the gateway.

```mermaid
sequenceDiagram
    participant A as Web App (SQLite)
    participant GW as Gateway
    participant B as Agent (SQLite)
    participant Source as Any Data Source

    A->>GW: push local changes
    GW-->>A: ACK + pull remote deltas
    B->>GW: pull (sync rule: errors, last 24h)
    GW->>Source: query via adapter
    Source-->>GW: filtered results
    GW-->>B: deltas matching rule
    Note over GW,Source: Adapters are sources AND destinations
    GW->>Source: flush / archive via adapter
```

## Adapters

Any data source you can read from becomes a LakeSync adapter. Adapters are both sources and destinations — enabling bidirectional sync and cross-backend flows.

| Adapter | Interface | Use case |
|---------|-----------|----------|
| **Postgres / MySQL** | `DatabaseAdapter` | Operational OLTP data, familiar SQL tooling |
| **BigQuery** | `DatabaseAdapter` | Analytics-scale queries, managed and serverless |
| **S3 / R2 (Iceberg)** | `LakeAdapter` | Massive scale on object storage, open format |
| **Jira Cloud** | API Connector | Sync issues, comments, and projects from Jira |
| **Anything else** | Either interface | CloudWatch, Stripe, custom APIs — implement the interface |

The `DatabaseAdapter` interface (`insertDeltas`, `queryDeltasSince`, `getLatestState`, `ensureSchema`) handles SQL-like sources. The `LakeAdapter` interface (`putObject`, `getObject`, `listObjects`, `deleteObject`) handles object storage. The `CompositeAdapter` routes data to multiple backends simultaneously.

## Hybrid Logical Clocks (HLC)

Every mutation is timestamped with an `HLCTimestamp` — a branded `bigint` encoding 48 bits of wall-clock time and 16 bits of a monotonic counter. This provides:

- **Causal ordering** across clients without centralised coordination
- **Deterministic tiebreaking** — when timestamps are equal, the higher `clientId` wins
- **Compact representation** — a single 64-bit value instead of a timestamp + counter pair

```ts
import { HLC, type HLCTimestamp } from "lakesync";

const hlc = new HLC();
const ts: HLCTimestamp = hlc.now();
```

## Delta Model

A **Delta** represents a single mutation to a single row. Rather than storing full row snapshots, LakeSync stores _column-level_ changes:

```ts
interface Delta {
  deltaId: string;       // SHA-256 of stable-stringified payload
  table: string;
  rowId: string;
  columns: Record<string, unknown>;
  hlc: HLCTimestamp;
  clientId: string;
}
```

Deterministic `deltaId` generation ensures that the same logical change always produces the same identifier, enabling idempotent processing.

## Conflict Resolution

LakeSync uses **column-level last-writer-wins (LWW)**:

1. For each column in a delta, compare the incoming HLC with the stored HLC for that column
2. If the incoming HLC is greater, the incoming value wins
3. If HLCs are equal, the higher `clientId` wins (deterministic tiebreak)
4. Columns not present in the incoming delta are left untouched

This means two clients can edit different columns of the same row concurrently without either change being lost.

```mermaid
sequenceDiagram
    participant A as Client A
    participant GW as Gateway
    participant B as Client B

    A->>A: UPDATE title = "Draft"
    B->>B: UPDATE status = "done"
    A->>GW: push delta (title, HLC=100)
    B->>GW: push delta (status, HLC=101)
    Note over GW: Column-level merge<br/>title ← A (HLC 100)<br/>status ← B (HLC 101)
    GW->>A: pull (status = "done")
    GW->>B: pull (title = "Draft")
    Note over A,B: Both changes preserved
```

## Result Pattern

Public APIs never throw. Instead, they return `Result<T, E>`:

```ts
import { ok, err, type Result } from "lakesync";

function divide(a: number, b: number): Result<number, string> {
  if (b === 0) return err("Division by zero");
  return ok(a / b);
}

const result = divide(10, 2);
if (result.ok) {
  console.log(result.value); // 5
} else {
  console.error(result.error);
}
```

## DeltaBuffer

The gateway stores deltas in a `DeltaBuffer` — a dual data structure with:

- **Log** — an append-only array of deltas in insertion order (for pull)
- **Index** — a map from `table:rowId` to the latest delta per column (for conflict resolution)

## Adapter-Sourced Pull

The gateway can pull data from named source adapters in addition to its in-memory buffer. A consumer declares what it needs (e.g. "give me errors from the last 24h from BigQuery") and the gateway queries the adapter, applies sync rules, and returns filtered deltas.

```ts
// Client-side: pull from a named source adapter
const coordinator = new SyncCoordinator(db, transport);
await coordinator.pullFrom("bigquery");
```

```mermaid
sequenceDiagram
    participant C as Consumer (SQLite)
    participant GW as Gateway
    participant BQ as BigQuery Adapter
    participant PG as Postgres Adapter

    C->>GW: pull (source: "bigquery", rules: errors only)
    GW->>BQ: queryDeltasSince(hlc)
    BQ-->>GW: all deltas
    Note over GW: Apply sync rules filter
    GW-->>C: filtered error deltas
    C->>GW: pull (source: "postgres")
    GW->>PG: queryDeltasSince(hlc)
    PG-->>GW: all deltas
    GW-->>C: deltas
    Note over C: Local SQLite has data from both sources
```

Source adapters are registered on the gateway via `sourceAdapters` in `GatewayConfig`. The `source` field on `SyncPull` selects which adapter to query. When omitted, the existing in-memory buffer path is used (backwards compatible).

## Sync Rules

Sync rules define which data each client can see. Rules are organised into **buckets** with filter operators:

| Operator | Description | Example |
|----------|-------------|---------|
| `eq` | Equals (exact match) | `{ "op": "eq", "value": "jwt:sub" }` |
| `in` | Contained in (multi-value) | `{ "op": "in", "value": "jwt:roles" }` |
| `neq` | Not equals | `{ "op": "neq", "value": "archived" }` |
| `gt` | Greater than | `{ "op": "gt", "value": "100" }` |
| `lt` | Less than | `{ "op": "lt", "value": "50" }` |
| `gte` | Greater than or equal | `{ "op": "gte", "value": "0" }` |
| `lte` | Less than or equal | `{ "op": "lte", "value": "1000" }` |

Comparison operators (`gt`, `lt`, `gte`, `lte`) use numeric comparison when both values parse as numbers, falling back to string comparison via `localeCompare()`.

```json
{
  "buckets": [{
    "name": "user-data",
    "filters": [
      { "column": "user_id", "op": "eq", "value": "jwt:sub" }
    ],
    "tables": ["todos", "preferences"]
  }]
}
```

The `jwt:` prefix references claims from the client's JWT token. At pull time, the gateway evaluates sync rules against the client's token claims and returns only matching deltas.

```mermaid
sequenceDiagram
    participant A as Alice (team=eng)
    participant GW as Gateway
    participant B as Bob (team=sales)

    Note over GW: Sync rules configured:<br/>filter todos by jwt:team
    A->>GW: pull (JWT: team=eng)
    GW-->>A: eng todos only
    B->>GW: pull (JWT: team=sales)
    GW-->>B: sales todos only
    Note over A,B: Each client sees only their data
```

## Real-Time Sync

When a client pushes deltas, the gateway broadcasts them to all other connected WebSocket clients immediately — filtered by each client's sync rules. HTTP polling remains as a fallback.

```mermaid
sequenceDiagram
    participant A as Client A (WS)
    participant GW as Gateway
    participant B as Client B (WS)

    A->>GW: push delta
    GW-->>A: SyncResponse
    GW-->>B: broadcast (< 100ms)
    Note over B: Applied via same LWW<br/>conflict resolution as pull
```

The `WebSocketTransport` uses the same binary protobuf protocol as HTTP with tag-based framing (`0x01` push, `0x02` pull, `0x03` broadcast). It reconnects automatically on disconnect with exponential backoff. When realtime is active, polling drops to a 60-second heartbeat to catch missed deltas.

See [Real-Time Sync](/docs/real-time) for setup and configuration details.

## Source Polling Ingest

External databases (Neon, Postgres, MySQL) and APIs (Jira) may have data written by services outside LakeSync. The **Source Polling Ingest** system bridges that gap — it periodically queries source tables or APIs, detects changes, and pushes them through the gateway as deltas.

```mermaid
sequenceDiagram
    participant DB as Source DB
    participant SP as SourcePoller
    participant GW as Gateway
    participant C as Client

    SP->>DB: poll every N seconds
    DB-->>SP: changed rows
    SP->>GW: handlePush (direct)
    GW-->>C: broadcast / pull
```

Two change detection strategies are available for database sources: **cursor** (fast, requires a monotonically increasing column like `updated_at`) and **diff** (slower, detects hard deletes via full-table comparison). API connectors like Jira use their own strategies internally. All pollers run inside `GatewayServer` — no HTTP hop, no auth needed. Detected changes flow through the same conflict resolution and sync rules as client-originated deltas.

See [Source Polling Ingest](/docs/source-polling) for database polling setup and [Dynamic Connectors](/docs/connectors) for API connectors like Jira.

## Checkpoints

For initial sync (when `lastSyncedHlc === 0`), the client downloads a **checkpoint** — a complete snapshot of the current state. Checkpoints are:

- Generated post-compaction from base Parquet files
- Encoded as Protocol Buffer chunks, one per table
- Sized to a byte budget (default 16 MB per chunk)
- Filtered at serve time using the client's sync rules (the stored checkpoint contains all rows)

```mermaid
sequenceDiagram
    participant C as New Client
    participant GW as Gateway
    participant ICE as Iceberg

    C->>GW: pull (lastSyncedHlc = 0)
    Note over GW: First sync — serve checkpoint
    GW->>ICE: read base Parquet files
    ICE-->>GW: checkpoint chunks
    Note over GW: Filter by client's sync rules
    GW-->>C: checkpoint (proto-encoded)
    Note over C: Hydrate local SQLite
    C->>GW: pull (lastSyncedHlc = checkpoint HLC)
    GW-->>C: incremental deltas since checkpoint
    Note over C: Fully synced ✓
```
